## Daily Schedule Part 2 (Actual &mdash; Kept Retrospectively)

*Regular meeting schedule is Wednesdays and Saturdays, 11:00-12:00*

Back to [Course home page](./index.html)

See also [Daily Schedule - Part 1](./daily_schedule_part1.html)

### Part 2: Data Science Foundations (using Joel Grus, *Data Science from Scratch, 2nd Edition*)

Part 2 Uses Grus and lasts for the remaining three-and-a-half weeks of Term 6

#### Week 4 &mdash; Yet Another Review of Python &mdash; Some Vector and Matrix Algebra &mdash; Statistics and Probability

* June 4 &mdash; Chapters 1-3: Another excellent review of Python and Matplotlib which will help systematize your understanding of the language features you were using in Pasha's book &mdash; The assignment is to do the review of the three chapters, but to completely stop using Jupyter or Jupyter lab, and instead get everything working in PyCharm Professional Edition (free for students) or VS Code (but I have zero experience with that) &mdash; When Grus says (at the beginning of Chapter 2) that you should not be tampering with your base Python environment, he is completely correct (so learn how to make a venv that you could call grus or dsfs and then switch to it &mdash; if you didn't already do that for working through Pasha)
* June 7 &mdash; Chapters 4-6: Linear Algebra (wherein Grus introduces his Vector and Matrix implementations which could have been classes, or could have leveraged numpy, but which he craftily used type aliases, because that was the simplest way to implement from scratch), Statistics, and Probability (due to having taken last fall's Bayesian Statistics class, the math in Chapters 5 and 6 will be review)

#### Week 5 &mdash; Optimization (aka Minimization and Maximization) &mdash; Working with Data

* June 11 &mdash; Chapters 7 and 8: Hypotheses &amp; Inference and Gradient Descent &mdash; Make a local repo from the magic hexijin.github.io GitHub repo, put an index.md file in it, and then push to origin main &mdash; The only remaining step to having [your own home page](https://hexijin.github.io) is to enable GitHub pages in this repo &mdash; For more advanced reading, Grus recommends this [Overview of Gradient Descent](https://www.ruder.io/optimizing-gradient-descent/) by Eric Ruder
* June 15 &mdash; Chapters 9 and 10: Getting and Working with Data

#### Week 6 &mdash; Machine Learning &mdash; Neural Networks &mdash; Start Deep Learning

* June 17 &mdash; Chapters 11 and 13: Machine Learning and Naive Bayes &mdash; Since classes are something we haven't done much with, but they are on your coding assessment, find some quick introduction to classes, such as Section 1.16 of [Python Distilled](https://www.dabeaz.com/python-distilled/) by Beazley &mdash; The three things that are generally considered important about classes are encapsulation (controlled access to data), inheritance (behavior is inherited from superclasses in the class heirarchy and extended by subclasses), and polymorphism (behavior can be overriden in subclasses) &mdash; Chapter 7 of Beazley is a much more complete introduction to classes
* June 21 &mdash; Chapters 18 and 19: Neural Networks and Deep Learning &mdash; Do this live coding session to see how a real pro codes, including type-hinting, systematic adherence to style choices, and code testing: [Joel Grus - Building a Deep Learning Library](https://joelgrus.com/2017/12/04/livecoding-madness-building-a-deep-learning-library/) (build the code in PyCharm as Grus builds it in VS Code, pausing the live coding demonstration whenever you need to catch up with him) &mdash; This live coding session is effectively a blindingly-fast version of Chapters 18 and 19

#### Week 7 &mdash; Continue Deep Learning &mdash; Introduction to Natural Language Processing

* June 25 &mdash; Chapter 21: Natural Language Processing &mdash; Watch the fifth of the 3Blue1Brown videos, [Transformers Explained Visually](https://youtu.be/wjZofJX0v4M), by Grant Sanderson numbered DL1 to DL7 &mdash; "DL" is short for "Deep Learning," and the seven videos were published from 2017 to 2024 &mdash; The fifth video gives you alook into the 2017 transformers revolution, and at what you would study next if you want to keep getting closer to the state of the art of machine learning and LLMs

See also [Looking Beyond](./looking_beyond.html)
