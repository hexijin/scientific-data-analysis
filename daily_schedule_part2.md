## Daily Schedule Part 2 (Actual &mdash; Kept Retrospectively)

*Regular meeting schedule is Wednesdays and Saturdays, 11:00-12:00*

Back to [Course home page](./index.html)

See also [Daily Schedule - Part 1](./daily_schedule_part1.html)

### Part 2: Data Science Foundations (using Joel Grus, *Data Science from Scratch, 2nd Edition*)

Part 2 Uses Grus and lasts for the remaining three-and-a-half weeks of Term 6

#### Week 4 &mdash; Yet Another Review of Python &mdash; Some Vector and Matrix Algebra &mdash; Statistics and Probability

* June 4 &mdash; Chapters 1-3: Another excellent review of Python and Matplotlib which will help systematize your understanding of the language features you were using in Pasha's book &mdash; The assignment is to do the review of the three chapters, but to completely stop using Jupyter or Jupyter lab, and instead get everything working in PyCharm Professional Edition (free for students) or VS Code (but I have zero experience with that) &mdash; When Grus says (at the beginning of Chapter 2) that you should not be tampering with your base Python environment, he is completely correct (so learn how to make a venv that you could call grus or dsfs and then switch to it &mdash; if you didn't already do that for working through Pasha)

#### === BELOW THIS DIVISION IS GOAL/TENTATIVE PLAN &mdash; NOT ACTUAL ===

* June 7 &mdash; Chapters 4-6: Linear Algebra (wherein Grus introduces his Matrix and Vector classes), Statistics, and Probability (thanks to last fall's Bayesian Statistics class, everything in Chapters 5 and 6 will be review) &mdash; [Ch. 4 Matrix Algebra](./grus/grus04.ipynb)

#### Week 5 &mdash; Optimization (aka Minimization and Maximization) &mdash; Working with Data

* June 11 &mdash; Chapters 7 and 8: Hypotheses &amp; Inference and Gradient Descent
* June 14 &mdash; Chapters 9 and 10: Getting and Working with Data

#### Week 6 &mdash; Machine Learning &mdash; Neural Networks &mdash; Start Deep Learning

* June 18 &mdash; Chapters 11 and 13: Machine Learning and Naive Bayes
* June 21 &mdash; Chapters 18 and 19: Neural Networks and Deep Learning

#### Week 7 &mdash; Continue Deep Learning &mdash; Introduction to Natural Language Processing

* June 25 &mdash; A great finale for Part 2 of the IS would be to (1) Do this real-time coding session to see how a real pro codes, including type-hinting, adherence to systematic style choices, and code testing (you build the code with Grus, pausing the video whenever you need to catch up, because he is fast): [Joel Grus - Building a Deep Learning Library](https://joelgrus.com/2017/12/04/livecoding-madness-building-a-deep-learning-library/) and then (2) Do Chapter 21, Natural Language Processing

#### Looking Beyond

The chapter we finished with does not attempt to cover the 2017 &ldquo;Attention is All You Need&rdquo; transformers revolution &mdash; Nonetheless, Grus has almost perfectly set you up for a second-semester course in deep learning and LLMs that almost any computer science department will (or soon will) be offering.

You can survey beyond where Grus has taken us in two ways:

(1) Consider how LLMs have changed and will continue to change the workflow of a data scientist by watching from the 15:00 mark in [Doing Data Science in the Time of ChatGPT](https://youtu.be/oyV81rnLSJc?t=900) by Grus. This is a very casual survey that may only serve to cement what you have already discovered you can do with a high-end LLM like Grok 3 or ChatGPT 4.5.

(2) Recapitulate what we have done and then look inside the mathematics and implementation of LLMs, without actually implementing any part of them, by watching the seven 3Blue1Brown videos by Grant Sanderson numbered DL1 to DL7. DL is short for "Deep Learning," and the seven videos were published from 2017 to 2024. You will find you can very easily skip the first few of Sanderson's videos given what we have already learned from Grus, so consider starting with the fifth video in the series: [Transformers Explained Visually](https://youtu.be/wjZofJX0v4M).
